Search.setIndex({"alltitles": {"0.1.0": [[0, "id11"]], "0.1.1": [[0, "id10"]], "0.2.0": [[0, "id9"]], "0.2.1": [[0, "id8"]], "0.3.0": [[0, "id7"]], "0.3.1": [[0, "id6"]], "0.3.2": [[0, "id5"]], "0.4.0": [[0, "id4"]], "0.4.1": [[0, "id3"]], "0.4.2": [[0, "id2"]], "0.5.0": [[0, "id1"]], "API reference": [[3, null]], "Basic usage": [[4, null]], "Change log": [[0, null]], "Cleaning": [[3, "cleaning"], [4, "cleaning"]], "Contents": [[1, "contents"]], "Dependencies": [[2, "dependencies"]], "Finding locations within large strings": [[3, "finding-locations-within-large-strings"]], "Geo": [[3, "geo"], [4, "geo"]], "Installation": [[2, null]], "Normalization": [[3, "normalization"]], "Note": [[4, null], [4, null], [4, null]], "Resource loading": [[4, "resource-loading"]], "Resources": [[3, "resources"]], "TextScrubber": [[3, "textscrubber"], [4, "textscrubber"]], "text-scrubber documentation": [[1, null]]}, "docurls": ["changelog.html", "index.html", "install.html", "reference.html", "usage.html"], "envversion": {"sphinx": 63, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1}, "indexentries": {"__init__() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.__init__", false]], "__repr__() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.__repr__", false]], "__str__() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.__str__", false]], "__weakref__ (text_scrubber.text_scrubber.textscrubber attribute)": [[3, "text_scrubber.text_scrubber.TextScrubber.__weakref__", false]], "add_city_resources() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.add_city_resources", false]], "add_region_resources() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.add_region_resources", false]], "clean_city() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.clean_city", false]], "clean_country() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.clean_country", false]], "clean_region() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.clean_region", false]], "convert_html_entities() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.convert_html_entities", false]], "extractedlocation (class in text_scrubber.geo.find_in_string)": [[3, "text_scrubber.geo.find_in_string.ExtractedLocation", false]], "filter_tokens() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.filter_tokens", false]], "find_city_in_string() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.find_city_in_string", false]], "find_country_in_string() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.find_country_in_string", false]], "find_region_in_string() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.find_region_in_string", false]], "fix_bad_unicode() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.fix_bad_unicode", false]], "initials() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.initials", false]], "join() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.join", false]], "latex_to_text() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.latex_to_text", false]], "location (class in text_scrubber.geo.normalize)": [[3, "text_scrubber.geo.normalize.Location", false]], "lowercase() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.lowercase", false]], "normalize_city() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.normalize_city", false]], "normalize_country() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.normalize_country", false]], "normalize_country_to_country_codes() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.normalize_country_to_country_codes", false]], "normalize_region() (in module text_scrubber.geo)": [[3, "text_scrubber.geo.normalize_region", false]], "normalize_unicode() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.normalize_unicode", false]], "num2words() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.num2words", false]], "remove_digits() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_digits", false]], "remove_excessive_whitespace() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_excessive_whitespace", false]], "remove_html_tags() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_html_tags", false]], "remove_punctuation() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_punctuation", false]], "remove_quotes() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_quotes", false]], "remove_stop_words() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_stop_words", false]], "remove_suffixes() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.remove_suffixes", false]], "removes_prefixes() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.removes_prefixes", false]], "sort() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sort", false]], "strip() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.strip", false]], "strip_accents() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.strip_accents", false]], "sub() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sub", false]], "sub_greek_chars() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sub_greek_chars", false]], "sub_html_chars() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sub_html_chars", false]], "sub_latex_chars() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sub_latex_chars", false]], "sub_tokens() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.sub_tokens", false]], "text_transform() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.text_transform", false]], "textscrubber (class in text_scrubber.text_scrubber)": [[3, "text_scrubber.text_scrubber.TextScrubber", false]], "to_ascii() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.to_ascii", false]], "to_list() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.to_list", false]], "token_transform() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.token_transform", false]], "tokenize() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.tokenize", false]], "transform() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.transform", false]], "transform_generator() (text_scrubber.text_scrubber.textscrubber method)": [[3, "text_scrubber.text_scrubber.TextScrubber.transform_generator", false]]}, "objects": {"text_scrubber.geo": [[3, 0, 1, 0, "add_city_resources", "Read and parse city resources for new countries"], [3, 0, 1, 0, "add_region_resources", "Read and parse region resources for new countries"], [3, 0, 1, 0, "clean_city", "Cleans a strings with geographical information (e.g., countries/regions/cities)."], [3, 0, 1, 0, "clean_country", "Cleans a strings with geographical information (e.g., countries/regions/cities)."], [3, 0, 1, 0, "clean_region", "Cleans a strings with geographical information (e.g., countries/regions/cities)."], [3, 0, 1, 0, "find_city_in_string", "Extracts cities from a sample text."], [3, 0, 1, 0, "find_country_in_string", "Extracts countries from a sample text."], [3, 0, 1, 0, "find_region_in_string", "Extracts regions from a sample text."], [3, 0, 1, 0, "normalize_city", "Cleans up a city by string cleaning and performs city lookups to get the canonical name"], [3, 0, 1, 0, "normalize_country", "Cleans up a country by string cleaning and performs some basic country lookups to get the canonical name."], [3, 0, 1, 0, "normalize_country_to_country_codes", "Normalizes countries or country codes to the set of corresponding country codes. E.g., 'Denmark' will result in {'DK', 'FO', 'GL'} for Denmark, Faroe Islands, and Greenland"], [3, 0, 1, 0, "normalize_region", "Cleans up a region by string cleaning and performs region lookups to get the canonical name"]], "text_scrubber.geo.add_city_resources": [[3, 1, 2, 0, "country_codes", "Set of country codes."], [3, 1, 2, 0, "progress_bar", "disable or enable progressbar."]], "text_scrubber.geo.add_region_resources": [[3, 1, 2, 0, "country_codes", "Set of country codes."], [3, 1, 2, 0, "progress_bar", "disable or enable progressbar."]], "text_scrubber.geo.clean_city": [[3, 1, 2, 0, "string", "Input string to clean."]], "text_scrubber.geo.clean_country": [[3, 1, 2, 0, "string", "Input string to clean."]], "text_scrubber.geo.clean_region": [[3, 1, 2, 0, "string", "Input string to clean."]], "text_scrubber.geo.find_city_in_string": [[3, 1, 2, 0, "country_set", "Restrict the search to this set of countries"], [3, 1, 2, 0, "match_threshold", "threshold for considering a substring a match"], [3, 1, 2, 0, "match_threshold_small", "threshold for considering a substring a match, applied to smaller normalized countries"], [3, 1, 2, 0, "max_tokens_to_consider", "maximum amount of tokens to consider as a combination for comparing to normalized countries"], [3, 1, 2, 0, "sample", "text sample"], [3, 1, 2, 0, "threshold_small", "if the length of a candidate string is <= threshold_small it will use the match_threshold_small, otherwise match_threshold"]], "text_scrubber.geo.find_country_in_string": [[3, 1, 2, 0, "match_threshold", "threshold for considering a substring a match"], [3, 1, 2, 0, "match_threshold_small", "threshold for considering a substring a match, applied to smaller normalized countries"], [3, 1, 2, 0, "max_tokens_to_consider", "maximum amount of tokens to consider as a combination for comparing to normalized countries"], [3, 1, 2, 0, "sample", "text sample"], [3, 1, 2, 0, "threshold_small", "if the length of a candidate string is <= threshold_small it will use the match_threshold_small, otherwise match_threshold"]], "text_scrubber.geo.find_in_string": [[3, 2, 1, 0, "ExtractedLocation", ""]], "text_scrubber.geo.find_in_string.ExtractedLocation.__init__": [[3, 1, 2, "text_scrubber.geo.find_in_string.ExtractedLocation", "location", ""], [3, 1, 2, "text_scrubber.geo.find_in_string.ExtractedLocation", "substring", ""], [3, 1, 2, "text_scrubber.geo.find_in_string.ExtractedLocation", "substring_range", ""]], "text_scrubber.geo.find_region_in_string": [[3, 1, 2, 0, "country_set", "Restrict the search to this set of countries"], [3, 1, 2, 0, "match_threshold", "threshold for considering a substring a match"], [3, 1, 2, 0, "match_threshold_small", "threshold for considering a substring a match, applied to smaller normalized countries"], [3, 1, 2, 0, "max_tokens_to_consider", "maximum amount of tokens to consider as a combination for comparing to normalized countries"], [3, 1, 2, 0, "sample", "text sample"], [3, 1, 2, 0, "threshold_small", "if the length of a candidate string is <= threshold_small it will use the match_threshold_small, otherwise match_threshold"]], "text_scrubber.geo.normalize": [[3, 2, 1, 0, "Location", ""]], "text_scrubber.geo.normalize.Location.__init__": [[3, 1, 2, "text_scrubber.geo.normalize.Location", "canonical_name", ""], [3, 1, 2, "text_scrubber.geo.normalize.Location", "country", ""], [3, 1, 2, "text_scrubber.geo.normalize.Location", "matched_name", ""], [3, 1, 2, "text_scrubber.geo.normalize.Location", "score", ""]], "text_scrubber.geo.normalize_city": [[3, 1, 2, 0, "city", "City name"], [3, 1, 2, 0, "min_score_levenshtein", "minimum score to use for Levenshtein similarity"], [3, 1, 2, 0, "min_score_trigram", "minimum score to use for trigram similarity"], [3, 1, 2, 0, "restrict_countries", "A set of countries and/or country codes to restrict the search space"]], "text_scrubber.geo.normalize_country": [[3, 1, 2, 0, "country", "Country string to clean."], [3, 1, 2, 0, "min_score_levenshtein", "Minimum score to use for Levenshtein similarity"], [3, 1, 2, 0, "min_score_trigram", "Minimum score to use for trigram similarity"]], "text_scrubber.geo.normalize_country_to_country_codes": [[3, 1, 2, 0, "countries", "Set of countries or country codes"]], "text_scrubber.geo.normalize_region": [[3, 1, 2, 0, "min_score_levenshtein", "Minimum score to use for Levenshtein similarity"], [3, 1, 2, 0, "min_score_trigram", "Minimum score to use for trigram similarity"], [3, 1, 2, 0, "region", "Region name"], [3, 1, 2, 0, "restrict_countries", "A set of countries and/or country codes to restrict the search space"]], "text_scrubber.text_scrubber": [[3, 2, 1, 0, "TextScrubber", "Cleans a single or a collection of strings."]], "text_scrubber.text_scrubber.TextScrubber": [[3, 3, 1, 0, "__init__", ""], [3, 3, 1, 0, "__repr__", "Displays the cleaning pipeline."], [3, 3, 1, 0, "__str__", "Displays the cleaning pipeline."], [3, 4, 1, 0, "__weakref__", "list of weak references to the object (if defined)"], [3, 3, 1, 0, "convert_html_entities", "Converts HTML entities to their corresponding characters using Python's html parser. Unlike sub_html_chars() this also converts named entities like &amp;."], [3, 3, 1, 0, "filter_tokens", "Filter tokens given a certain test."], [3, 3, 1, 0, "fix_bad_unicode", "Fixes common unicode issues using the ftfy library, e.g. mojibake like \u00c3\u00a9 are changed to \u00e9. See https://ftfy.readthedocs.io/en/latest/index.html for what ftfy will fix and how to configure it."], [3, 3, 1, 0, "initials", "Removes all, but the first element of a text or tokens."], [3, 3, 1, 0, "join", "Joins elements to a single string."], [3, 3, 1, 0, "latex_to_text", "Converts LaTeX markup to text using the pylatexenc library. See https://pylatexenc.readthedocs.io/en/latest/ Unlike sub_latex_chars() this removes all LaTeX markup, not just commands, and converts mathematical formulas to text as well, at the cost of performance."], [3, 3, 1, 0, "lowercase", "Lowercases text or tokens."], [3, 3, 1, 0, "normalize_unicode", "Normalizes a unicode string using python's unicodedata module. See https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize"], [3, 3, 1, 0, "num2words", "Converts numbers like 42 to words like forty-two. See https://github.com/savoirfairelinux/num2words."], [3, 3, 1, 0, "remove_digits", "Removes digits from text or tokens."], [3, 3, 1, 0, "remove_excessive_whitespace", "Replaces multi-character whitespace with a single whitespace."], [3, 3, 1, 0, "remove_html_tags", "Removes all HTML tags (i.e., everything that matches r'<.*?>')."], [3, 3, 1, 0, "remove_punctuation", "Removes all, but the provided punctuation from text or tokens."], [3, 3, 1, 0, "remove_quotes", "Removes single and double quotes from text or tokens."], [3, 3, 1, 0, "remove_stop_words", "Removes stop words. All-caps tokens are considered to be abbreviations and are retained."], [3, 3, 1, 0, "remove_suffixes", "Removes a set of suffixes."], [3, 3, 1, 0, "removes_prefixes", "Removes a set of prefixes."], [3, 3, 1, 0, "sort", "Sort text or tokens."], [3, 3, 1, 0, "strip", "Uses the builtin str.strip function to strip leading/trailing characters from text or tokens."], [3, 3, 1, 0, "strip_accents", "Uses anyascii to strip accents and convert unicode characters to plain 7-bit ASCII."], [3, 3, 1, 0, "sub", "Replace all occurrences of a search query with a replacement string."], [3, 3, 1, 0, "sub_greek_chars", "Replace Greek characters with their English name. E.g., \u03b1 is replaced by alpha."], [3, 3, 1, 0, "sub_html_chars", "Match HTML char encodings such as &#410; and replaces them with the equivalent unicode character."], [3, 3, 1, 0, "sub_latex_chars", "Match accented LaTeX commands such as \\\"{o} and \\^{\\i} OR match direct commands such as \\o and \\l{} and {\\L}, and keep only the regular ascii character."], [3, 3, 1, 0, "sub_tokens", "Substitutes tokens."], [3, 3, 1, 0, "text_transform", "Adds a function operating on texts to the pipeline."], [3, 3, 1, 0, "to_ascii", "Uses anyascii to strip accents and convert unicode characters to plain 7-bit ASCII."], [3, 3, 1, 0, "to_list", "Converts the iterable of tokens to a list (useful when not joining as you're otherwise left with generator or filter objects which still need to be evaluated)."], [3, 3, 1, 0, "token_transform", "Adds a function operating on tokens to the pipeline."], [3, 3, 1, 0, "tokenize", "Tokenizes a text."], [3, 3, 1, 0, "transform", "Transform a single or multiple strings."], [3, 3, 1, 0, "transform_generator", "Transform multiple strings and receive a generator as result."]], "text_scrubber.text_scrubber.TextScrubber.convert_html_entities": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "whether to transform on a list of tokens or a single string"]], "text_scrubber.text_scrubber.TextScrubber.filter_tokens": [[3, 1, 2, "text_scrubber.text_scrubber.TextScrubber.filter_tokens", "name=&#x27;filter_tokens&#x27;", "Filter tokens given a certain test."], [3, 1, 2, "text_scrubber.text_scrubber.TextScrubber.filter_tokens", "neg=False", "Filter tokens given a certain test."]], "text_scrubber.text_scrubber.TextScrubber.filter_tokens.test=&lt;function TextScrubber": [[3, 1, 2, "text_scrubber.text_scrubber.TextScrubber.filter_tokens", "&lt;lambda&gt;&gt;", "Filter tokens given a certain test."]], "text_scrubber.text_scrubber.TextScrubber.fix_bad_unicode": [[3, 1, 2, 0, "config", "A ftfy.TextFixerConfig instance for configuring what ftfy fixes or None to use defaults."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.initials": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.join": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."], [3, 1, 2, 0, "sep", "Separator to join tokens."]], "text_scrubber.text_scrubber.TextScrubber.latex_to_text": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string"]], "text_scrubber.text_scrubber.TextScrubber.lowercase": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.normalize_unicode": [[3, 1, 2, 0, "form", "The unicode normalization form to use."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.num2words": [[3, 1, 2, 0, "include_commas", "Whether to let num2words include commas for more natural reading."], [3, 1, 2, 0, "language", "The language in which to convert the number (see num2words documentation for possible values)."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_digits": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_excessive_whitespace": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_html_tags": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_punctuation": [[3, 1, 2, 0, "keep_punctuation", "A string containing the punctuation-tokens that should NOT be removed."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_quotes": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.remove_stop_words": [[3, 1, 2, 0, "case_sensitive", "should the stopword removal be case_sensitive or not"], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "stop_words", "Iterable of stop words to remove."]], "text_scrubber.text_scrubber.TextScrubber.remove_suffixes": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."], [3, 1, 2, 0, "suffixes", "Set of suffixes to remove."]], "text_scrubber.text_scrubber.TextScrubber.removes_prefixes": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."], [3, 1, 2, 0, "prefixes", "Set of prefixes to remove."]], "text_scrubber.text_scrubber.TextScrubber.sort": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."], [3, 1, 2, 0, "reverse", "Reverse sort."]], "text_scrubber.text_scrubber.TextScrubber.strip": [[3, 1, 2, 0, "chars", "If chars is given and not None, remove leading/trailing characters in chars instead of whitespace."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.strip_accents": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.sub": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."], [3, 1, 2, 0, "replace", "Replacement string or callable for matched groups."], [3, 1, 2, 0, "search", "String or regex on which to match."]], "text_scrubber.text_scrubber.TextScrubber.sub_greek_chars": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.sub_html_chars": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.sub_latex_chars": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.sub_tokens": [[3, 1, 2, 0, "func", "Function that substitutes tokens."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.text_transform": [[3, 1, 2, 0, "func", "Function to perform on whole strings."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.to_ascii": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."], [3, 1, 2, 0, "on_tokens", "Whether to transform on a list of tokens or a single string."]], "text_scrubber.text_scrubber.TextScrubber.to_list": [[3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.token_transform": [[3, 1, 2, 0, "func", "Function to perform on each token."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.tokenize": [[3, 1, 2, 0, "func", "Function used for tokenizing a string."], [3, 1, 2, 0, "name", "Name to give to the pipeline step."]], "text_scrubber.text_scrubber.TextScrubber.transform": [[3, 1, 2, 0, "on_tokens", "Whether to treat the iterable of strings as tokens or complete strings."], [3, 1, 2, 0, "s", "One or multiple texts, which can either be tokenized or not."], [3, 1, 2, 0, "to_set", "Whether to return a set instead of a list when an iterable of strings is provided."]], "text_scrubber.text_scrubber.TextScrubber.transform_generator": [[3, 1, 2, 0, "s", "Iterable of strings."]]}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "parameter", "Python parameter"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"], "4": ["py", "attribute", "Python attribute"]}, "objtypes": {"0": "py:function", "1": "py:parameter", "2": "py:class", "3": "py:method", "4": "py:attribute"}, "terms": {"": 3, "0": [3, 4], "02": 0, "03": 0, "04": 0, "05": 0, "08": 0, "09": 0, "1": 4, "10": [0, 4], "100871": 4, "103": 4, "114": 4, "12": 4, "13": [0, 4], "14": [0, 4], "17": 4, "19": [0, 4], "20": 4, "2020": 0, "2021": 0, "2022": 0, "2023": 0, "2024": 0, "22": 4, "26": 0, "27": 0, "29": 4, "3": [3, 4], "34": 4, "39": 4, "4": 3, "40": 0, "410": 3, "42": 3, "45": 4, "5": [3, 4], "56": 4, "59": 4, "6": [0, 3], "60": 4, "61": 4, "63": 4, "65": 4, "7": [0, 3], "75": 4, "7884": 4, "8": 3, "84": 3, "857": 4, "888": 4, "9": 3, "909": 4, "92": 4, "923": 4, "95030": 4, "97": 4, "A": 3, "For": 4, "If": [3, 4], "It": 4, "NOT": 3, "OR": 3, "One": 3, "The": [0, 3, 4], "There": 4, "__init__": 3, "__repr__": 3, "__str__": 3, "__weakref__": 3, "abbrevi": 3, "about": 0, "abov": 4, "accent": 3, "accra": 4, "ad": 0, "add": 3, "add_city_resourc": [3, 4], "add_region_resourc": [3, 4], "advanc": 4, "afr": 4, "africa": 4, "african": 4, "agenc": 4, "ai": [0, 4], "all": [0, 3, 4], "allow": 0, "alpha": 3, "also": [0, 3, 4], "altern": 4, "alwai": 4, "amount": 3, "amp": 3, "an": [3, 4], "anoth": [3, 4], "anyascii": [0, 2, 3], "api": [1, 4], "appli": 3, "ar": [0, 2, 3, 4], "aren": 4, "around": 4, "ascii": 3, "attribut": 4, "austria": 4, "automat": 2, "avail": 4, "base": 3, "basic": [1, 3], "bavaria": 4, "bavarian": 4, "becaus": 0, "been": 0, "beij": 4, "between": 4, "biolog": 4, "bit": 3, "block": [1, 3, 4], "bool": 3, "boolean": 0, "both": 4, "brazil": 4, "build": [1, 2, 3, 4], "builtin": 3, "callabl": 3, "can": [0, 2, 3, 4], "candid": 3, "canon": [0, 3, 4], "canonical_nam": [3, 4], "cap": 3, "case_sensit": [0, 3], "cent": 4, "central": 4, "certain": 3, "ch": 4, "chang": [1, 3], "char": 3, "charact": 3, "china": 4, "citi": [0, 1, 3, 4], "class": [3, 4], "clean": [0, 1], "clean_citi": [0, 3, 4], "clean_countri": [0, 3, 4], "clean_region": [0, 3, 4], "cn": 4, "code": [0, 3], "collect": [3, 4], "com": 3, "combin": 3, "comma": 3, "command": 3, "common": [0, 3], "compar": 3, "complet": [0, 3, 4], "config": 3, "configur": [3, 4], "consid": [3, 4], "construct": [3, 4], "contain": [3, 4], "convert": 3, "convert_html_ent": [0, 3], "correspond": [0, 3, 4], "cost": 3, "countr": 4, "countri": [0, 1, 3, 4], "country_cod": [3, 4], "country_set": 3, "cython": [0, 2], "data": 4, "databas": 0, "deal": 4, "default": 3, "defin": 3, "denmark": [3, 4], "depart": 4, "depend": 0, "deriv": 3, "desc": 3, "deutschland": 4, "differ": 4, "digit": 3, "direct": 3, "disabl": 3, "displai": 3, "distribut": 2, "district": 4, "dk": 3, "do": 4, "doc": 3, "document": [0, 3], "doubl": 3, "e": [3, 4], "each": [3, 4], "easili": [3, 4], "either": 3, "element": 3, "empir": 3, "en": 3, "enabl": 3, "encod": 3, "end": 4, "english": 3, "entiti": 3, "environ": 4, "equival": 3, "er": 4, "error": 4, "etc": 4, "evalu": 3, "everyth": 3, "exist": 4, "expand": 0, "expect": 0, "extract": 3, "extractedloc": [0, 3, 4], "fact": 4, "fals": 3, "faro": 3, "faster": 4, "file": 0, "filter": 3, "filter_token": 3, "find": [0, 4], "find_city_in_str": [0, 3, 4], "find_country_in_str": [0, 3, 4], "find_in_str": [0, 3], "find_region_in_str": [0, 3, 4], "first": [0, 3, 4], "fix": 3, "fix_bad_unicod": [0, 3], "flag": 0, "fly": [0, 4], "fo": 3, "follow": 4, "form": 3, "formula": 3, "forti": 3, "found": 4, "franc": 4, "frasn\u00e9": 4, "from": [0, 3, 4], "ftfy": [2, 3], "func": 3, "function": [0, 1, 3, 4], "fur": 4, "fuzzi": 0, "g": [3, 4], "gener": 3, "geo": 0, "geograph": [1, 3, 4], "geonam": 0, "german": 4, "germani": 4, "get": 3, "ghana": 4, "github": [0, 3], "give": 3, "given": 3, "gl": 3, "greek": 3, "greenland": [3, 4], "group": 3, "ha": 0, "haidian": 4, "han": 4, "handl": 0, "have": [0, 4], "heidelberg": 4, "hello": 4, "hof": 4, "how": 3, "html": 3, "http": 3, "hyogo": 4, "hy\u014dgo": 4, "h\u00e9ll\u00f4": 4, "h\u00f6gn": 4, "i": [0, 1, 3, 4], "import": 4, "improv": 0, "includ": [0, 3], "include_comma": 3, "increas": 0, "index": 3, "indic": 0, "inform": [0, 3], "initi": 3, "input": [3, 4], "instal": 1, "instanc": 3, "instead": 3, "institut": 4, "integ": 0, "involv": 4, "io": 3, "ira": 4, "iran": 4, "iraq": 4, "island": 3, "issu": [0, 3], "iter": [3, 4], "join": [3, 4], "just": 3, "keep": 3, "keep_punctu": 3, "kind": 0, "kingdom": 4, "kitt": 4, "kitti": 4, "known": 4, "l": 3, "la": 4, "lambda": 3, "languag": 3, "latest": 3, "latex": 3, "latex_to_text": [0, 3], "latter": [3, 4], "lead": 3, "left": 3, "leibnitz": 4, "length": 3, "let": 3, "levenshtein": [0, 3], "librari": 3, "licens": 0, "like": [3, 4], "list": [3, 4], "load": 0, "locat": [0, 4], "locationmatch": [3, 4], "log": 1, "lookup": 3, "lowercas": [3, 4], "madri": 4, "madrid": 4, "manifest": 0, "map": [0, 4], "mari": 4, "markup": 3, "match": [0, 3, 4], "match_threshold": 3, "match_threshold_smal": 3, "matched_nam": [3, 4], "mathemat": 3, "max_tokens_to_consid": 3, "maximum": 3, "mean": 4, "min_score_levenshtein": 3, "min_score_trigram": 3, "minimum": 3, "modul": [3, 4], "mojibak": 3, "more": [0, 3], "move": 0, "much": 4, "multi": 3, "multipl": 3, "museum": 4, "m\u00e9t\u00e9orag": 4, "name": [0, 3, 4], "natur": 3, "need": 3, "neg": 3, "netherland": 4, "neustadt": 4, "nevi": 4, "new": [3, 4], "nfc": 3, "nfd": 3, "nfkc": 3, "nfkd": 3, "nioz": 4, "nl": 4, "none": [3, 4], "normal": [0, 1, 4], "normalize_c": [0, 3, 4], "normalize_countri": [0, 3, 4], "normalize_country_to_country_cod": [3, 4], "normalize_region": [0, 3, 4], "normalize_st": 0, "normalize_unicod": [0, 3], "note": 3, "now": 0, "num2word": [2, 3], "number": 3, "numpi": 2, "nyc": 4, "nycc": 4, "o": 3, "object": [0, 3, 4], "occurr": 3, "oceanographi": 4, "offer": 1, "offici": 3, "ohio": 4, "ohioo": 4, "on_token": 3, "one": 3, "onli": 3, "oper": 3, "optim": 0, "org": 3, "other": 4, "otherwis": 3, "our": 3, "output": [0, 4], "ownership": 0, "packag": [1, 2], "paramet": 3, "park": 4, "pars": 3, "parser": 3, "part": [3, 4], "pau": 4, "peke": 4, "peopl": 4, "per": 4, "perfect": 4, "perform": 3, "pip": 2, "pipelin": [0, 3, 4], "place": 4, "plain": 3, "pleas": 4, "plonsk": 4, "possibl": 3, "prefix": 3, "prevent": 0, "progress_bar": [3, 4], "progressbar": 3, "provid": [1, 3], "puerto": 4, "punctuat": 3, "pylatexenc": [2, 3], "pypi": [0, 2], "python": [0, 1, 2, 3], "p\u0142o\u0144sk": 4, "queri": [3, 4], "question": 4, "quot": 3, "r": 3, "rang": 4, "rapidfuzz": [0, 2], "rd": 4, "re": [3, 4], "read": 3, "readthedoc": 3, "reason": 0, "receiv": 3, "refer": [1, 4], "regex": 3, "region": [0, 3, 4], "regular": 3, "relax": 0, "releas": 0, "remov": [0, 3], "remove_digit": 3, "remove_excessive_whitespac": 3, "remove_html_tag": 3, "remove_punctu": 3, "remove_quot": 3, "remove_stop_word": [0, 3, 4], "remove_suffix": 3, "removes_prefix": 3, "renam": 0, "rep": 4, "replac": [0, 3], "repositori": 0, "republ": 4, "research": 4, "restrict": 3, "restrict_countri": 3, "result": 3, "retain": 3, "return": [0, 3, 4], "revers": 3, "rico": 4, "royal": 4, "saal": 4, "saint": 4, "same": 4, "sampl": 3, "savoirfairelinux": 3, "scipi": 2, "score": [0, 3, 4], "scrub": 1, "scrubber": [0, 2], "sea": 4, "search": 3, "second": 4, "see": 3, "sep": 3, "separ": 3, "set": 3, "should": 3, "similar": [0, 3], "singl": [3, 4], "slimmer": [0, 4], "sl\u00edmm": 4, "smaller": 3, "some": [3, 4], "sort": 3, "sourc": 3, "south": 4, "space": 3, "spain": 4, "speed": 0, "spell": 4, "st": 4, "start": 4, "state": [1, 4], "stem": 4, "step": 3, "still": 3, "stop": 3, "stop_word": 3, "stopword": 3, "str": 3, "stra\u00dfe": 4, "string": [0, 1, 4], "string_transform": 3, "strip": 3, "strip_acc": 3, "strip_excessive_whitespac": 3, "strip_prefix": 3, "strip_quot": 3, "strip_suffix": 3, "studi": 4, "sub": 3, "sub_greek_char": 3, "sub_html_char": 3, "sub_latex_char": 3, "sub_token": 3, "substitut": 3, "substitute_token": 3, "substr": [3, 4], "substring_rang": [3, 4], "suffix": 3, "support": 0, "switzerland": 4, "sybrenjansen": 0, "t": [3, 4], "tag": 3, "take": 4, "test": 3, "texel": 4, "text": [0, 2, 3, 4], "text_scrubb": [0, 3, 4], "text_transform": 3, "textfixerconfig": 3, "textscrubb": 0, "thei": [0, 4], "them": 3, "thi": [3, 4], "though": 4, "threshold": 3, "threshold_smal": 3, "through": [0, 2], "time": 4, "to_ascii": [0, 3, 4], "to_list": 3, "to_set": 3, "togeth": 0, "token": [3, 4], "token_transform": 3, "tqdm": 2, "trail": 3, "transform": [3, 4], "transform_gener": 3, "trassem": 4, "treat": 3, "triangl": 4, "trick": 4, "trigram": [0, 3], "true": 4, "two": 3, "type": 3, "u": 4, "unicod": 3, "unicodedata": 3, "unidecod": 0, "union": 3, "unit": 4, "univers": 4, "unlik": 3, "up": [0, 3], "updat": 0, "upload": 0, "us": [3, 4], "usa": 4, "usag": 1, "usecas": 3, "valid": 3, "valu": 3, "variat": 4, "wa": 4, "we": 3, "weak": 3, "well": [0, 1, 3], "were": 3, "westerwald": 4, "what": 3, "when": [2, 3, 4], "whenev": 4, "where": 4, "whether": 3, "which": [0, 3, 4], "while": 4, "whitespac": 3, "whole": 3, "word": 3, "world": 4, "w\u00f2rld": 4, "x20": 0, "y": 4, "yiheyuan": 4, "york": 4, "you": [3, 4], "\u00e0i": 4, "\u00e3": 3, "\u00e9": 3, "\u03b1": 3, "\u0444\u0440\u0430\u043d\u0435": 4}, "titles": ["Change log", "text-scrubber documentation", "Installation", "API reference", "Basic usage"], "titleterms": {"0": 0, "1": 0, "2": 0, "3": 0, "4": 0, "5": 0, "api": 3, "basic": 4, "chang": 0, "clean": [3, 4], "content": 1, "depend": 2, "document": 1, "find": 3, "geo": [3, 4], "instal": 2, "larg": 3, "load": 4, "locat": 3, "log": 0, "normal": 3, "note": 4, "refer": 3, "resourc": [3, 4], "scrubber": 1, "string": 3, "text": 1, "textscrubb": [3, 4], "usag": 4, "within": 3}})